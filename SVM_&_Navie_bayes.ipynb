{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1 : What is Information Gain, and how is it used in Decision Trees?**\n",
        "-\n",
        "Information Gain (IG) measures the reduction in entropy or impurity after splitting a dataset based on a particular feature. It quantifies how well a feature separates the training examples according to their target classes.\n",
        "\n",
        "Formula:\n",
        "\n",
        "IG(D,A)=Entropy(D)‚àív‚ààValues(A)‚àë‚Äã‚à£D‚à£‚à£Dv‚Äã‚à£‚Äã√óEntropy(Dv‚Äã)\n",
        "\n",
        "where\n",
        " - D = dataset\n",
        " - A = attribute\n",
        " - ùê∑ùë£Dv= subset of ùê∑D where attributeùê¥=ùë£A=v\n",
        "\n",
        "Use in Decision Trees:\n",
        "Decision trees (like ID3 and C4.5) use Information Gain to decide which attribute to split the data on at each node. The attribute with the highest information gain is chosen as the decision node because it gives the purest (least uncertain) subsets.\n",
        "\n",
        "**Question 2: What is the difference between Gini Impurity and Entropy?**\n",
        "-\n",
        "| Feature            | **Gini Impurity**                                    | **Entropy**                                            |\n",
        "| ------------------ | ---------------------------------------------------- | ------------------------------------------------------ |\n",
        "| **Formula**        | (1 - \\sum p_i^2)                                     | (- \\sum p_i \\log_2 p_i)                                |\n",
        "| **Range**          | 0 to 0.5                                             | 0 to 1                                                 |\n",
        "| **Interpretation** | Measures the probability of incorrect classification | Measures the amount of disorder or uncertainty         |\n",
        "| **Computation**    | Simpler and faster                                   | Slightly more complex (involves logarithm)             |\n",
        "| **Used in**        | CART algorithm                                       | ID3, C4.5 algorithms                                   |\n",
        "| **Preference**     | Works well in most cases, less computational cost    | Provides more theoretical understanding of information |\n",
        "\n",
        "**Question 3:What is Pre-Pruning in Decision Trees?**\n",
        "-\n",
        "Pre-Pruning (also called early stopping) prevents overfitting by halting tree growth before it perfectly classifies all training data.\n",
        "\n",
        " - Methods include:\n",
        "\n",
        "-Limiting maximum tree depth (max_depth)\n",
        "\n",
        "-Minimum samples required to split a node (min_samples_split)\n",
        "\n",
        "-Minimum samples per leaf (min_samples_leaf)\n",
        "\n",
        "-Setting a minimum information gain threshold\n",
        "\n",
        " - Advantages:\n",
        "\n",
        "-Reduces overfitting\n",
        "\n",
        "-Improves model generalization\n",
        "\n",
        "-Lowers computation time\n",
        "\n",
        "**Question 5: What is a Support Vector Machine (SVM)?**\n",
        "-\n",
        "Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression tasks. It finds the optimal hyperplane that separates different classes with the maximum margin.\n",
        "\n",
        " - Key Concepts:\n",
        "\n",
        "Support Vectors: Data points closest to the hyperplane.\n",
        "\n",
        "Margin: Distance between hyperplane and support vectors.\n",
        "\n",
        "Objective: Maximize margin to improve generalization.\n",
        "\n",
        " - Advantages:\n",
        "\n",
        "Effective in high-dimensional spaces.\n",
        "\n",
        "Works well with clear margin of separation.\n",
        "\n",
        "Can use kernels for non-linear classification.\n",
        "\n",
        "\n",
        "**Question 6: What is the Kernel Trick in SVM?**\n",
        "-\n",
        "The Kernel Trick allows SVMs to perform non-linear classification by transforming data into a higher-dimensional space without explicitly computing coordinates in that space.\n",
        "\n",
        " - Common Kernels:\n",
        "\n",
        "Linear Kernel:K(x,y) = x‚Ä¢y\n",
        "\n",
        "Polynomial Kernel:K(x,y) = (x‚Ä¢y+c)\n",
        "\n",
        "RBF (Gaussian) Kernel: K(x,y) = exp(-y|x - y[¬≤)\n",
        "\n",
        " - Advantages:\n",
        "\n",
        "-Handles complex, non-linear data efficiently.\n",
        "\n",
        "-Reduces computation by using inner products instead of transformations.\n",
        "\n",
        "**Question 8: What is the Na√Øve Bayes classifier, and why is it called ‚ÄúNa√Øve‚Äù?**\n",
        "-\n",
        "Na√Øve Bayes is a probabilistic classifier based on Bayes‚Äô Theorem, assuming that all features are independent given the class label.\n",
        "\n",
        "It‚Äôs called ‚ÄúNa√Øve‚Äù because it assumes feature independence ‚Äî an assumption that is rarely true in real life but still works surprisingly well.\n",
        "\n",
        " - Advantages:\n",
        "\n",
        "-Simple and fast.\n",
        "\n",
        "-Works well for text classification and spam detection.\n",
        "\n",
        "-Requires small training data.\n",
        "\n",
        "**Question 9: Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes**\n",
        "-\n",
        "| Type               | Suitable For            | Assumes                                          | Example Use Case                                      |\n",
        "| ------------------ | ----------------------- | ------------------------------------------------ | ----------------------------------------------------- |\n",
        "| **Gaussian NB**    | Continuous data         | Features follow a normal (Gaussian) distribution | Iris dataset, medical data                            |\n",
        "| **Multinomial NB** | Discrete counts         | Feature values are counts/frequencies            | Text classification (bag of words)                    |\n",
        "| **Bernoulli NB**   | Binary/boolean features | Features take values 0 or 1                      | Spam detection, sentiment (presence/absence of words) |\n",
        "\n",
        "Choice depends on the nature of feature values (continuous, counts, or binary)."
      ],
      "metadata": {
        "id": "tOZJ90PWI_qu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEitvqSgIHOE",
        "outputId": "7759ff29-0d40-46c2-ff4a-f90a5429c1be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sepal length (cm): 0.0133\n",
            "sepal width (cm): 0.0000\n",
            "petal length (cm): 0.5641\n",
            "petal width (cm): 0.4226\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Question 4:Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\"\"\"\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = iris.target\n",
        "\n",
        "# Train Decision Tree using Gini Impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "for name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Question 7: Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies.\n",
        "\"\"\"\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "wine = load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    wine.data, wine.target, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train SVM with Linear and RBF kernels\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "\n",
        "svm_linear.fit(X_train, y_train)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "acc_linear = accuracy_score(y_test, svm_linear.predict(X_test))\n",
        "acc_rbf = accuracy_score(y_test, svm_rbf.predict(X_test))\n",
        "\n",
        "print(f\"Linear Kernel Accuracy: {acc_linear:.2f}\")\n",
        "print(f\"RBF Kernel Accuracy: {acc_rbf:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hyeZWT0M3Sw",
        "outputId": "02b798cf-eb06-4648-82b4-c555dd7269d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 0.98\n",
            "RBF Kernel Accuracy: 0.76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Question 10: Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n",
        "\"\"\"\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.data, data.target, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = gnb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Gaussian Naive Bayes Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCz62hWSOBAx",
        "outputId": "761e6ff4-e2ac-4b07-db7d-4c5657bf76c5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Naive Bayes Accuracy: 0.94\n"
          ]
        }
      ]
    }
  ]
}